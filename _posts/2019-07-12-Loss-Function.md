---
title: Loss Function
date: 2019-07-12
categories: Deep-Learning
tags:
- math
- loss functions
- cross entropy loss

---

　　一段时间之后总是会对损失函数这部分的内容有所遗忘，需要时不时地进行复习。
<!-- more -->

　　损失函数的作用是用来评估一个模型的好坏与否，从目前的认知来看，损失函数能够有效给模型添加几何约束和数学的依据，损失函数的设计对于网络模型的设计来说也是至关重要的一个环节。

## 均方误差
　　均方误差是一个比较常见的损失函数，均方损失函数的定义为：

$$
L=\frac{1}{n}\sum_{i}^{n}(\hat{y_i}-y_i)^2
$$

## 交叉熵损失函数
　　对于二分类问题来说，交叉熵损失函数的表达式等于：

$$
L=-[y\times\log{(p)}+(1-p)\times\log{(1-p)}]
$$

　　其中，y表示的是样本的label，如果是正样本，那么y的值等于1，也就是要分别计算两个类别各自对应估计的概率，简单推算一下便可以知道，如果标签为正的类别估计出来的概率等于0，那么对应的损失函数的值将会由log(0)非常大，反应出来我们的模型设计的并不合理。

　　在多分类的问题上，交叉熵损失函数的表达式等于：

$$
L=-\sum_{c=1}^{M}y_c\log{p_c}
$$

　　其中M表示的是类别的数量，y<sub>c</sub>表示当前预测的类别和样本真实的标签是相同的，那么这个索引index的值等于，否则等于0，其中p<sub>c</sub>表示预测类别c的概率。通过对交叉熵损失函数的分析，实际上，该函数只是对groundtruth等于1的那个分类情况进行判断，因为其他的索引门控index=0的情况下，计算出来的结果也是等于0的。

## 逻辑回归等

　　跟着交叉熵损失函数伴随而来的还有一堆的线性回归和逻辑回归、softmax啊等等的。接下来对这些概念再次进行梳理。<br
　　逻辑回归的作用是用来解决二分类问题的机器学习算法，用于估计某个事件的可能性，从知乎上的讲解来看，逻辑回归之后的输出并不是直接当做概率来使用。<br>
那么逻辑回归和线性回归之间又是什么一个概念呢。依然是摘自知乎，逻辑回归假设变量服从伯努利分布，线性回归假设变量服从高斯分布。在之前的理解内，逻辑回归就是在线性回归的基础上然后添加sigmoid函数，将输出投影到[0,1]之间。如果去除sigmoid函数，逻辑回归就是一个线性回归。逻辑回归通过sigmoid函数引入了非线性因素，可以轻松处理二分类的01问题。

$$
sigmoid=\frac{1}{1+e_(-x)}\\\
softmax=\frac{e^{y_c}}{\sum_{i=1}^{M}{e^{y_i}}}
$$

　　softmax逻辑回归模型是逻辑回归在处理多分类问题上的推广，softmax函数，称为归一化指数函数，是逻辑函数的一种推广。样本x属于第j个类别的概率等于：

$$
P(y=j|x)=\frac{e^{x^Tw_j}}{\sum_{k=1}^{K}e^{x^Tw_k}}
$$

　　然后可以在softmax的基础上再配上交叉熵损失函数。