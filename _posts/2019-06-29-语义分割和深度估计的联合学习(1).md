---
title: 语义分割和深度估计的联合学习(1)
date: 2019-06-29
categories: Deep-Learning
tags:
- depth estimation
- semantic segamentation
- multi-task learning
---

　　这篇论文是南京大学和多伦多大学合作发表的一篇论文，任务是进行深度估计、表面法向量和语义分割的联合任务学习，提出了cross-task propagation和task-specific propagation两种传播方法。

<!--more-->

## 联合学习存在的问题

　　多任务联合学习其实使用在近些年感觉还是挺热门的一个研究方向的，自己的毕业设计也是抱着尝鲜的心态，选择了做目标检测和深度估计的多任务学习，虽然最后勉强算是实现了功能，实现了一个网络模型能够对目标检测任务和深度估计任务进行预测。

　　自己在选择尝试多任务学习这个题目的时候，出发点是能够借助深度学习的优势，相对较为容易的实现两个任务的融合，有效减少模型参数的数量，提高模型的实时性。在多次毕业设计答辩环节，老师有问一个问题，问什么要选择多任务学习的方式来解决这两个任务，单个任务的效果难道不是比你做这两个任务结合起来的效果更好吗？从最后的实验结果来看，确实在单个任务上的表现要更好一些。之前在看一本书的时候里面写道，多任务学习的方式一方面能够有效扩充数据量，另一方面能够使得不同任务之间可以相互受益，这一点是通过共享底层特征来实现的。最开始做的时候，就是凭借着最初读过的MultiNet这篇论文给出的网络结构，想着将目标检测和深度估计两个任务也设计为这这样一个编码器-解码器结构的网络模型。然而自己的设计过于粗暴，直接将深度估计的编码器部分的网络结构用VGG16来替换，然后强行修改了网络的输入分辨率，借助着fine-tune的思想，冻结其中一个分支之后，训练另一个分支，用着自己标注的数据集和Kitti的双目数据集交替去训练这两个网络的分支。

　　现在来看自己好像确实没有做什么东西，这个感觉就像保送之后重新回顾自己曾经做过的语义分割任务，没有什么自己的原创内容嘛。在做的过程中想着，通过将深度信息引入到目标检测任务中，说不定能够有效提高目标检测任务的识别精度，但是从实验最后的结果来看，这个想法并没有实现，目标检测任务的精度并没有较大程度上的直接提升，深度估计任务有着肉眼可见的效果下降。在进行工作交接的时候，同事问我，我这个工作之后的改进方向或是说问题存在在哪，我想了想，当时想出来了下面三个方面：

1. 首先不应该暴力地直接将深度估计任务的编码器网络结构用VGG16来代替，选择的300*300分辨率现在来看是比较小的，所包含的特征有限。
2. 并没有掌握多任务学习训练的方法，多任务的模型设计的也太草率，多任务模型中底层特征共享的这个优势并没有利用好。现在反思一个很重要的问题：目标检测和深度估计两个任务真的可以设计为多任务模型吗？二者的底层特征真的是可以共享吗？
3. 数据集的问题，用到的两个数据集并不是同一个场景下的数据集，深度估计任务的数据集直接使用了Kitti的双目数据集，目标检测任务使用的是自己标注的数据集，不知道是不是这个原因使得深度估计的特征不能帮助目标检测任务在效果上得到提升。

　　今天看论文的时候，作者对当前的这个多任务学习方法提出的存在问题，个人觉得是非常具有参考价值的：However, most methods aimed to perform fusion or parameter sharing for task interaction. The fusion or sharing ways may utilize the correlative information between tasks,  but there exist some drawbacks. For examples, the integration of different features might result into the ambiguity of information; the fusion does not explicitly model the task-level interaction where we do not know what information are transmitted. 

　　作者说到，不同特征之间的合成可能会导致信息的模糊不清，这样的融合并没有明确指出模型的各个任务之间的相互作用关系，我们也无法确定有哪些信息被传递了。在自己做实验的这个过程中，确实发现了，直接将网络的某些层进行共享之后，训练的过程中，感觉像是揉成了一团，共享底层特征训练到什么样的一种程度并不知道。共享的方法虽然从功能上来说实现了多任务，但是从效果上来说，感觉缺少一些修正或是改善项，所以表现并不理想。

　　对于多任务学习的问题，其实是迁移学习的一种，迁移学习有一个顺序的要求，需要先学习任务A，然后迁移到任务B。在多任务学习中，我们一开始就使用一个神经网络去同时完成多个任务，并且希望这些任务里的每一个任务都能帮助到另一个任务，那么我们应该什么时候来使用呢：

- 在一系列任务上进行训练，它们有共享的低层特征，这使得任务之间相互获益。
- 常见用例：每一个任务的数据量过小。
- 可以训练一个足够大的网络使得它在所有的任务上都能够表现良好。

　　**关键点在于，我们使用的网络必须要足够大，在这个条件下多任务学习才不会对总体性能产生有害影响。**

## Affinity Learning

　　对于这个概念，之前是没有接触过的，然而整个论文似乎又是以这个概念为基础展开的，所以还是需要花时间来整理这个概念。

### Affinity Matrix

　　科学上网搜索这个矩阵的概念，Deepai给出了介绍：

　　**What is an Affinity Matrix?**

　　An Affinity Matrix, also called a Similarity Matrix, is an essential statistical technique used to organize the mutual similarities between a set of data points.  Similarity is similar to distance, however, it does not satisfy the properties of a metric, two points that are the same will have a similarity score of 1, whereas computing the metric will result in zero.  Typical examples of similarity measures are the [cosine similarity](https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity) and the Jaccard similarity.  These similarity measures can be interpreted as the [probability](https://deepai.org/machine-learning-glossary-and-terms/probability) that that two points are related. for example, if two data points have coordinates that are close, then their cosine similarity score ( or respective “affinity” score) will be much closer to 1 than two data points with a lot of space between them.

　　个人感觉翻译为相似度矩阵是合理的，是一个用于计算一组数据点之间共同相似程度的基本统计方法。相似程度和距离这个概念很相似，但是它并不满足一个度量标准应该具备的属性。两个相同的点，它们之间的距离等于0，但是它们的i相似度等于1。两个数据点越接近，它们的相似度越接近1。典型的相似度测量方法有cos similarity和Jaccard similarity，其实想一下在计算两个框的jaccard score的时候，如果两个框重合，jaccard值就等于1。cos similarity计算两个向量夹角的余弦值。![](D:\wyddmw.github.io\pic\cos_similarity.PNG)

