---
title: NNDL-Learning(1)
date: 2019-09-06
categories: Deep-Learning
tags:
- Theory Learning
- Network Optimization
---

最近要开始网络编写方面的内容，重点来弥补一下网络优化部分的基础知识，为了日后训练网络的时候能够取得更好的效果，所以这次的关键词是网络优化。
<!-- more -->

## 网络优化的难点

深层神经网络是一个高度非线性的模型，风险函数是一个非凸函数，因此风险最小化是一个非凸优化问题，会存在很多局部最优点。其实凸优化也是一个非常重要的研究方向，应用在深度学习方面，对网络的优化也有非常重要的意义。而网络优化的难点主要在于：

1. 网络结构多样性。神经网络的种类是非常多的，卷积网络、循环网络等，网络的结构也千差万别。不同参数在网络中的作用也有很大的差别。而且网络的超参数也非常多，对于网络的优化也并不是一个容易解决的问题。
2. 高维变量的非凸优化。低维空间的非凸优化问题主要是存在一些局部最优点。基于梯度下降的方法可能会使得陷入局部最优点。低维空间的非凸优化的主要难点是如何选择初始化参数和逃离局部最优点。但是对于高维空间来说，非凸优化的难点在于如何逃离鞍点。鞍点的梯度是零，但是在一些维度上是最高点，在一些维度上是最低点。在高维空间中，局部最优点要求在每一个维度上都是最低点，这种概率是非常低的。因为我们的网络参数非常多，维度非常高，所以在高维空间中，但部分梯度为零的点都是鞍点。基于梯度下降的方法会在鞍点附近停滞，很难逃离这些鞍点。



