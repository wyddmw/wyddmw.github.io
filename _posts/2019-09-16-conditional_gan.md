---
title: conditional_gan
date: 2019-09-16
categories: Gan-Learning
tags:
- 条件对抗生成模型
- pix2pix
---

　　之前看的gan模型中，所有的输入都是随机向量，但是这样的网络结构导致最终生成的结果是随机的，也就是我们无法控制生成的结果朝着我们希望的方向进行。但是在我们自己的网络模型任务中，我们需要通过给定的输入左视图，定向地生成对应的右视图，如果不添加条件的约束，显然是无法实现这样的效果的。所以补充条件生成网络模型的内容。<br>
　　因为我们最终的目标是通过左视图来生成右视图，基本的网络结构框架目前选择参考pix2pix的网络结构。

<!-- more -->

## conditional generative adversarial Nets
　　传统的生成对抗网络如果在生成器和判别器部分添加一些额外的条件信息y，那么对抗生成网络能够扩展为条件生成网络。y可以是任何辅助的信息，比如是类别的标签或是其他模态的信息。我们可以将y作为额外层添加到生成器和判别器中来引入条件约束。在生成器部分，输入的随机向量p(z)和额外的条件信息y进行了合并，得到一个联合隐式表达。在判别器部分x和y作为输入，之后送入到一个判别函数中，x和y通过MLP多层感知器进行合并。直接给出抽象的模型结构：![](/pic/cgan.png)
　　在生成器中，输入的噪声和作为条件信息的y会被合并为一个隐式表示hidden representation，对抗网络的训练框架对于这个隐式表达hidden representation是如何组成的有很大的灵活性。<br>
　　在作者给出的示例中，使用的是mnist数据集来完成这个任务，对应的条件信息是经过编码之后的one-hot格式的标签信息。

## pix2pix
　　pix2pix的这篇论文，目的是为了构建一个具有普适性的框架来完成不同任务间的从像素点到像素点的合成，主要使用的方法还是基于条件生成模型。为什么不直接使用cnn，然后给出一个简单的约束直接来进行图像的合成任务呢，作者在论文中指出，如果我们只是采用简单的方法然后使CNN去最小化合成得到的图像和真实图像之间的欧式距离的化，这样得到的合成视图使非常模糊的，这是因为欧式距离最小化的全部的可能的输出像素值，这就导致了最终生成图像的模糊。从基本的条件生成模型来看，生成模型的输入由两部分组成，一方面是随机的噪声，另一方面是代表条件的信息。<br>
　　在之前的一些工作中已经发现了，如果将gan的目标检测和一些更传统的损失函数如L2距离损失函数，这样做可以使得gan的效果更理想。判别器的任务并没有发生改变，依然是判断输入到判别器中的数据是来自真实数据还是生成的假的数据。所以在作者给出的论文中，最终使用的目标函数:
$$
G=argmin_Gmax_DL_{cGan}(G,D)+\Lambda{L_{L1}(G)}
$$

　　最终的作者选择使用L1范式的损失来进行额外的约束。

###  noise

　　作者在论文中说了一个非常重要的点，如果没有噪声z的输入，网络同样也能学习到从条件x到输出y的匹配映射，但是这样就会输出唯一的结果——然而有一个地方不清楚的是，根据stylegan中的说明，输入的噪声主要用来控制诸如头发发型、发色等变量，这些噪声的加入能够使得图像看起来更加自然，但是这样对于我们的任务来说是否是有帮助的呢？<br>

　　目前已经发现了，pix2pixHD网络的输出结果中，有一些是我们并不希望看到的，虽然从标签图生成了对应的真实图，首先得到的图和真实图像之间颜色是不同的，而且尽管都是生成了相同类别的物体，但是物体的形状并不完全相同，所以如果是这样的话，这对我们后期进一步做视察估计可能会带来更大的困难。

## 读完本文之后的感想

　　通篇论文读下来之后，感觉伯克利的这篇论文并没有什么很惊艳到我的地方，我指的是在网络结构方面。不管是生成器还是判别器，都还是中规中矩的网络结构，就是完全按照条件生成网络的原理走下来的，作者在论文中使用的例子是由cityscapes的标注信息去生成对应的真实图像，将标注信息视为条件信息，然后噪声在dropout处添加，生成器得到的合成图再结合着条件信息输入到判别器中，与真实的图像进行判别。在传统的生成网络中，噪声是直接输入到生成器中的，但是这里并没有完全输入到生成器中，按照作者在文中说的，只是在dropout处添加了噪声，这应该是第一个不同点。<br>

　　对我们的工作比较有启发的点是，作者在损失函数部分，添加了额外的L1范式作为约束，因为添加了传统的损失函数的约束之后，能够使输出和真实结果更加相近，而不仅仅是使生成的图像看上去更像是真实的图像，这一点和我们的出发点也是相吻合的，我们希望能够得到的左视图尽可能和真实的图像是相同的，从作者论文中给出的实验结果来看，确实添加了L1范式的损失函数之后，合成图的效果会更接近真实的视图，所以我们可以考虑在L1损失函数的基础上，进一步添加SSIM损失函数，和L1损失函数融合作为appearance matching cost但是在具体的细节以及颜色等方面还是有不同的地方的，可能这部分是添加了噪声之后的结果。<br>

　　除了上面损失函数的改善之外，作者在生成器的结构部分也通过实验来证明使用skip connection的效果要比单纯使用encoder-decoder得到的结果要好。

![](/pic/pix2pix.png)

![](/pic/pix2pix_2.png)

![](/pic/unet.png)

### 为什么我们的任务中可能不适合引入噪声的因素

　　以cityscapes为例，网络任务的输出对生成物体的颜色等内容并没有必要的需求，所以可以添加噪声，在一定程度上还能增加合成视图的多样性，现在来看这就是作者在论文中提到的为什么噪声因素是不能去掉的原因吧。