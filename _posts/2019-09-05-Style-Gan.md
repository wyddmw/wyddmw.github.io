---
title: Style-Gan
date: 2019-09-05
categories: Gan-Learning
tags:
- gan系列论文学习1
---

　　为了在我们的网络中生成得到更加逼真的左右视图，我们需要采用更加高效的网络结构；来进行视图生成的工作，而StyleGan则是在人脸生成模型中，效果非常出色的一个，对这个网络进行学习。<br>
<!-- more -->

## Introduction
　　对于图像的合成工作来说，尽管现在gan的效果已经非常好了，但是对于视图合成的很多不同方面至今仍然是一个黑盒子。一些潜在空间的并没有被很好的利用和理解。受到风格迁移的启发，作者探索了一种新颖的方法重新设计了生成器的网络结构尝试想要去控制图片合成的过程。整个网络结构只是在生成器部分的结构进行了修改，对于生成器部分的网络结构并没有进行改动，损失函数也是常规gan中的损失函数。作者提出的生成器，将输入的潜在（隐藏的）编码嵌入到中中间的隐藏的空间中，这样做对于变化因素在网络中的表示有着非常深远的意义。输入的潜在的空间必须要遵从训练集的概率密度分布。作者提出了两种自动化的量化方式——perceptual path length and linear separability——用来进一步量化生成器的这些方面（哪些方面？？？）。使用了这些量化之后，我们发现和传统结构的生成器相比，我们的生成器容许一个更线性、不那么混杂的改变因素的表示。<br>
　　StyleGan的直观网络结构如下：
![](/pic/StyleGan_Arch.png)
　　和传统的网络结构相比，网络并不是直接输入隐藏的编码，而是先由匹配网络f经过一系列的非线性变换，将输入转换到另一个隐层的空间中，然后将这个中间层得到的隐层编码结果再作为视图合成网络的输入。其中A代表的是一个经过学习的仿射变换。B是一种运算，给噪声在每一个通道上都添加学习过的尺度因子。

## 风格迁移
　　图像的风格就是特征图各个feature channel跨空间的统计信息。迁移各个channel的mean和variance就能够实现风格迁移。这里补充风格迁移的内容。图像的风格就是特征图各个feature channel跨空间的统计信息，比如说mean和variance。迁移各个channel的mean和variance就可以实现风格迁移。在之前的风格迁移的计算中，计算的流程如下，其中x和y代表的是空间坐标。

$$
G[i,j]=\sum_x^h\sum_y^w{F_i[x,y]\cdot{F_j[x,y]}}
$$

　　上述矩阵的运算本质上计算的就是特征图各个channel上的相关性。但是上述的这种方法需要循环optimization，所以生成一张图片的时间会比较长，所以后人提出使用前馈神经网络代替这个optimization的过程。也就是生成一个generator，这个generator中采用了BN的基本操作。如果将batch normalization替换成Instance Normalization，可以提高收敛的速度。其中BN和IN的区别在于mean和variance的计算，BN使用的是一个batch中所有图片进行统计的，而IN的mean和variance是从单张图片中计算出来的。其中$\gamma$和$\beta$对每个通道都是不一样的。使用不同的$\beta$和$\gamma$可以生成不同风格的图像。上面的这种方式用到的$\gamma$和$\beta$都是经过学习得到的，如果想要学习到新的风格，需要进行新的学习。而对于Adaptive Instance Normalization来说，是直接使用一张图像计算出风格变换需要的参数。

$$
IN(x)=\gamma(\frac{x-\mu(x)}{\sigma(x)})+\beta
$$

　　所以对于Adaptive Instance Normalization来讲，直接使用一张图像就能够计算得到$\beta$和$\gamma$，具体的计算如下，其中y是想学习风格的图像。

$$
AdaIn(x,y)=\sigma(y)(\frac{x-\mu(x)}{\sigma(x)})+\mu(y)
$$

　　从上面的公式中可以看到，我们希望学习到的风格来自图片y，分别学习到两组参数$y_s$和$y_b$也就是scale和bias，这两组参数都是基于各个通道的，所以就是对于想要迁移的风格，这里的每一个通道对应的都是要迁移的一个小的特征可能是表情或是姿势等。<br>

### Latent code
　　在常规的gan网络结构中，latent code主要用于生成器最初的输入，latent code就是一个随机向量，gan生成图像的随机性就在于latent code的随机性，所以为了更好地控制生成图片的特性，作者希望通过控制latent code来实现对生成图片的控制。在latent code中，每一个维度都对应了我们关注的特性，想改变图像的特征的时候就找到对应的维度调整latent code的值即可。这样的方式需要我们对latent code进行解耦操作，因为如果latent code中的每一个维度可能耦合了多个图像的特征，我们就很难直接对laten code进行对应的调整。当latent code只用于生成器的输入的时候，随着生成器层数的增加，它的影响会越来越小。<br>
　　而且在传统的gan网络结构中，latent code是直接作为网络的输入的，但是在这里作者是将latent code先转换到了一个中间隐层空间，得到一个intermediate latent code，接着这个intermediate latent code经非线性映射得到w，由这个w来对生成器生成的特征进行控制。<br>

### 解耦和
　　latent code作用于每一个卷积层之前有一个变换，可以实现解耦合，经过解耦合之后的latent code作用于每一层卷积层，借鉴递进的思想，这些层所影响/控制的特征是不同的，于是latent code对不同level的特征分别进行了控制。经过intermediate latent space得到的latent code经过仿射变换，就是网格结构中显示的A代表的内容，经过仿射变化得到的$y=(y_s,y_b)$可以用来表示style，用y来完成adaptive Instance normalization，这个操作是基于channel-wise的。风格迁移的目的就是保留content images的内容，用style image提取的style加到content image中。<br>
　　从latent space采样并进行非线性映射和仿射变换的过程可以视作是从已经学习到的分布中对某style图像采样，也就是每一层卷积后融入的style是不同的，最后生成器相当于从很多种style的集合中生成一张新的图像。这种方法使得每一种style产生的影响都是局部的（相当于是把styles解耦合了），这样就能够非常方便地操控各种图像的特征了。


## FID衡量图片的质量
　　如何衡量gan生成图片质量的好坏，用到的衡量标准是Fréchet Inception Distance距离。在之前用到的假设中，如果生成的图片质量越好，那么在对这张图片进行分类的时候得到某一个类的概率分数就越高，输出的概率分布函数图像就越尖锐。生成的图片多样性越强，那么类别的边缘分布就越平均，边缘分布的概率函数的图像就会越平整。但是这样的评价并不正确，因为如果生成的图片中含有多种类别的物体，在进行概率计算的过程中，得到的概率得分也会越平整。所以这种衡量的指标并不是合理的。<br>
　　FID则是计算真实图片和假图片在feature层面的距离，具体的公式如下：

$$
FID=||\mu_r-\mu_g||^2+T_r(\sum_r+\sum_g-2(\sum_r\sum_g)^{\frac{1}{2}})
$$

　　其中$\mu$分别代表的是真实图片和假图片的特征平均值，$\sum$代表的是特征的协方差矩阵。相比用InceptionNet计算得到的score这种方法来讲，通过分类的得分来比较图片的真实程度实际上是在将生成的图片和ImageNet中的图片进行比较。而FID因为是使用了全连接层之前的特征向量，所以用到的是InceptionNet提取特征的能力，并不会进行最后的分类判断，所以最终的结果会更合理。<br>

---
　　给出这篇论文的一个非常不错的解读链接[https://blog.csdn.net/lynlindasy/article/details/89555201](https://blog.csdn.net/lynlindasy/article/details/89555201)